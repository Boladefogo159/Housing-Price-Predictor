# ğŸ  House Prices - Advanced Regression Techniques (Kaggle)

Este projeto tem como objetivo prever os preÃ§os de venda de imÃ³veis residenciais com base em um conjunto de dados de Ames, Iowa â€” uma competiÃ§Ã£o clÃ¡ssica do Kaggle focada em regressÃ£o e prÃ©-processamento de dados.

---

## ğŸ“Œ Objetivo

Desenvolver um pipeline robusto e interpretÃ¡vel para prever o valor de venda (`SalePrice`) de imÃ³veis utilizando:

- Engenharia de features inteligente
- Modelagem preditiva com validaÃ§Ã£o cruzada
- OtimizaÃ§Ã£o de hiperparÃ¢metros
- Tratamento de heterocedasticidade
- SubmissÃ£o no Kaggle com resultados competitivos

---

## ğŸ§  Abordagem

### 1. PrÃ©-processamento
- ConversÃ£o de `SalePrice` para escala logarÃ­tmica com `np.log1p()` para tratar assimetria.
- RemoÃ§Ã£o e imputaÃ§Ã£o de valores ausentes com `SimpleImputer`.

### 2. Engenharia de Features (`FeatureEngineer`)
CriaÃ§Ã£o de variÃ¡veis para capturar padrÃµes de valor em casas mais caras, como:

- `TotalBathrooms`: Soma ponderada dos banheiros.
- `TotalSF`: Ãrea total Ãºtil (porÃ£o + andares superiores).
- `LuxoExterno`: PresenÃ§a de itens como piscina, deck, alpendre grande.
- `OverallQualCond`: Produto da qualidade geral com a condiÃ§Ã£o.
- `YearsSinceRemodel`: Tempo desde a Ãºltima reforma.

### 3. Pipelines com Scikit-Learn
Uso de `ColumnTransformer` com:

- `StandardScaler` para variÃ¡veis numÃ©ricas
- `OrdinalEncoder` para ordinais
- `OneHotEncoder` para variÃ¡veis nominais

---

## ğŸ” Modelos testados

| Modelo               | RMSE log (validaÃ§Ã£o) | PÃºblico Kaggle | ComentÃ¡rio |
|----------------------|----------------------|----------------|------------|
| Ridge Regression     | ~0.2292              | N/A            | Modelo base linear regularizado |
| Random Forest (bÃ¡sico) | ~0.1875            | ~0.180         | Melhora inicial com nÃ£o-linearidade |
| Random Forest + tuning + features | **~0.147** | **0.14766**   | Modelo final submetido |

> Obs: Modelos foram treinados com `GridSearchCV`, otimizando hiperparÃ¢metros como `n_estimators` e `max_depth`, busquei otimizar os demais hiperparÃ¢metros mas andei de lado com os resultados.

---

## ğŸ“ˆ Resultados (Kaggle Submission)

- âœ… Erro absoluto mÃ©dio: **~R$ 8.115,09**
- âœ… Erro percentual mÃ©dio: **~4.57%**
- âœ… Score pÃºblico no Kaggle: **0.14766**

Isso representa um modelo competitivo, com desempenho dentro do **top 15-25% da competiÃ§Ã£o**, mesmo sem uso de ensemble ou boosting.

---

## âš–ï¸ EstratÃ©gias de Robustez

- `sample_weight`: Testei dar mais peso a casas com valor acima de R$ 300 mil para tratar heterocedasticidade.
- `handle_unknown='ignore'`: Tratei diversas falhas do encoder ao lidar com categorias novas no `test.csv`.
> Obs: Ainda assim na submissÃ£o final hÃ¡ algum problema com encoder na column 0
---

## ğŸ“¦ Arquivos do Projeto

- `main.ipynb` â€“ Notebook com todo o pipeline de modelagem.
- `submission.csv` â€“ SubmissÃ£o gerada para o Kaggle.
- `FeatureEngineer()` â€“ Classe customizada para engenharia de variÃ¡veis.
- `README.md` â€“ DocumentaÃ§Ã£o e storytelling do projeto.
- `stepbystep.ipynb` - Demonstra o inÃ­cio do EDA.
---

## ğŸš€ Melhorias possÃ­veis

- Experimentar `HistGradientBoostingRegressor`, `XGBoost` ou `LightGBM`
- Implementar Stacking com Ridge, Random Forest e GBM
- Analisar erros residuais por regiÃ£o (`Neighborhood`) e outliers
- Avaliar uso de `Quantile Regression` para prever faixas de preÃ§os

## ğŸ¥¶ Principais desafios
- EDA e seleÃ§Ã£o de features, dataset robusto e amplo.
- Heterocedasticidade.
- Montar um modelo utilizando pipeline ğŸ“‹

---

## ğŸ§‘â€ğŸ’» Autor

Projeto conduzido por Jorge Silva Pereira, analista de dados e estudante de cientista de dados, engenharia de atributos e aplicaÃ§Ã£o prÃ¡tica de soluÃ§Ãµes de machine learning.

ğŸ“ [LinkedIn](https://www.linkedin.com/in/jorgepereira-/)

---
