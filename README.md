# House Prices - Advanced Regression Techniques (Kaggle)

Este projeto tem como objetivo prever os pre√ßos de venda de im√≥veis residenciais com base em um conjunto de dados de Ames, Iowa ‚Äî uma competi√ß√£o cl√°ssica do Kaggle focada em regress√£o e pr√©-processamento de dados.
Al√©m disso, aprender mais sobre regress√£o, pipeline e principalmente EDA em cima de um assunto que tenho pouca familiaridade.
---

## Objetivo

Desenvolver um pipeline robusto e interpret√°vel para prever o valor de venda (`SalePrice`) de im√≥veis utilizando:

- Engenharia de features inteligente
- Modelagem preditiva com valida√ß√£o cruzada
- Otimiza√ß√£o de hiperpar√¢metros
- Tratamento de heterocedasticidade
- Submiss√£o no Kaggle com resultados competitivos

---

## Abordagem

### 1. Pr√©-processamento
- Convers√£o de `SalePrice` para escala logar√≠tmica com `np.log1p()` 
> Obs: Para tratar assimetria, uma dica que est√° no tutorial do projeto.
- Remo√ß√£o e imputa√ß√£o de valores ausentes com `SimpleImputer`. 
> Obs: Como Guardrail para encoders diferentes que apareceram no test.csv

### 2. Engenharia de Features (`FeatureEngineer`)
Cria√ß√£o de vari√°veis para capturar padr√µes de valor em casas mais caras, como:

- `TotalBathrooms`: Soma ponderada dos banheiros.
- `TotalSF`: √Årea total √∫til (por√£o + andares superiores).
- `LuxoExterno`: Presen√ßa de itens como piscina, deck, alpendre grande.
- `OverallQualCond`: Produto da qualidade geral com a condi√ß√£o.
- `YearsSinceRemodel`: Tempo desde a √∫ltima reforma.
 > Obs: Foi a etapa que levou mais tempo, testei outras diversas vari√°veis mas que n√£o apresentaram correla√ß√£o direta suficiente por meio de testes de sele√ß√£o autom√°tica de features
### 3. Pipelines com Scikit-Learn 
Uso de `ColumnTransformer` com:

- `StandardScaler` para vari√°veis num√©ricas
- `OrdinalEncoder` para ordinais
- `OneHotEncoder` para vari√°veis nominais
> Obs: Biblioteca maravilhosa.
---

## Modelos testados

| Modelo               | RMSE log (valida√ß√£o) | P√∫blico Kaggle | Coment√°rio |
|----------------------|----------------------|----------------|------------|
| Ridge Regression     | ~0.2292              | N/A            | Modelo base linear regularizado |
| Random Forest (b√°sico) | ~0.1875            | ~0.180         | Melhora inicial com n√£o-linearidade |
| Random Forest + tuning + features | **~0.147** | **0.14766**   | Modelo final submetido |

> Obs: Modelos foram treinados com `GridSearchCV`, otimizando hiperpar√¢metros como `n_estimators` e `max_depth`, busquei otimizar os demais hiperpar√¢metros mas andei de lado com os resultados.

---

## Resultados (Kaggle Submission)

- Erro absoluto m√©dio: **~R$ 8.115,09**
- Erro percentual m√©dio: **~4.57%**
- Score p√∫blico no Kaggle: **0.14766**

Segundo o GPT esse modelo representa: **top 15-25% da competi√ß√£o**, mesmo sem uso de ensemble ou boosting.

---

## Estrat√©gias testadas

- `sample_weight`: Testei dar mais peso a casas com valor acima de R$ 300 mil para tratar heterocedasticidade mas aumentei o erro das casas com valores menores e acabei andando de lado.

---

## Arquivos do Projeto

- `main.ipynb` ‚Äì Notebook com todo o pipeline de modelagem.
- `submission.csv` ‚Äì Submiss√£o gerada para o Kaggle.
- `README.md` ‚Äì Documenta√ß√£o e storytelling do projeto.
- `stepbystep.ipynb` - Demonstra o in√≠cio do EDA.
---

## Sugest√µes de melhoria futura

- Experimentar `HistGradientBoostingRegressor`, `XGBoost` ou `LightGBM`


## Principais desafios
- EDA e sele√ß√£o de features, dataset robusto e amplo.
- Heterocedasticidade.
- Montar um modelo utilizando pipeline üìã

---

üìé [LinkedIn](https://www.linkedin.com/in/jorgepereira-/)

---
